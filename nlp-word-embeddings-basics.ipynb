{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install portalocker\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:01.806960Z","iopub.execute_input":"2024-02-05T06:43:01.807358Z","iopub.status.idle":"2024-02-05T06:43:19.021792Z","shell.execute_reply.started":"2024-02-05T06:43:01.807326Z","shell.execute_reply":"2024-02-05T06:43:19.020116Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting portalocker\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker\nSuccessfully installed portalocker-2.8.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:19.023710Z","iopub.execute_input":"2024-02-05T06:43:19.024097Z","iopub.status.idle":"2024-02-05T06:43:33.982707Z","shell.execute_reply.started":"2024-02-05T06:43:19.024048Z","shell.execute_reply":"2024-02-05T06:43:33.981390Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/nightly/cu118\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchinfo import summary\nimport numpy as np\nimport collections\n","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:33.984431Z","iopub.execute_input":"2024-02-05T06:43:33.984826Z","iopub.status.idle":"2024-02-05T06:43:39.568009Z","shell.execute_reply.started":"2024-02-05T06:43:33.984791Z","shell.execute_reply":"2024-02-05T06:43:39.566739Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# check the paltform, Apple Silicon or Linux\nimport os,platform\ntorch_device= \"cpu\"\nif 'kaggle' in os.environ.get('KAGGLE_URL_BASE','loaclhost'):\n    torch_device ='cuda'\nelse:\n    torch_device = 'mps' if platform.system()=='Darwin' else 'cpu'\ntorch_device","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:39.570841Z","iopub.execute_input":"2024-02-05T06:43:39.571441Z","iopub.status.idle":"2024-02-05T06:43:39.581789Z","shell.execute_reply.started":"2024-02-05T06:43:39.571406Z","shell.execute_reply":"2024-02-05T06:43:39.580596Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset,test_dataset = torchtext.datasets.AG_NEWS(root='./data')\ntrain_dataset,test_dataset = list(train_dataset),list(test_dataset)\nclasses = ['World','Sports','Business','Sci\\Tech']\n\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:39.583335Z","iopub.execute_input":"2024-02-05T06:43:39.583738Z","iopub.status.idle":"2024-02-05T06:43:45.810473Z","shell.execute_reply.started":"2024-02-05T06:43:39.583708Z","shell.execute_reply":"2024-02-05T06:43:45.809168Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def build_vocab(train_dataset,ngrams=1,min_freq=1):\n    counter = collections.Counter()\n    for (label,line) in train_dataset:\n        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line),ngrams=ngrams))\n    vocab = torchtext.vocab.vocab(counter,min_freq=1)\n    return vocab\nvocab = build_vocab(train_dataset, ngrams=1, min_freq=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:45.812089Z","iopub.execute_input":"2024-02-05T06:43:45.813372Z","iopub.status.idle":"2024-02-05T06:43:51.954287Z","shell.execute_reply.started":"2024-02-05T06:43:45.813323Z","shell.execute_reply":"2024-02-05T06:43:51.953389Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def encode(x,voc=None,tokenizer=tokenizer):\n    v =vocab   if not voc else voc\n    return [v.get_stoi()[s] for s in tokenizer(x)]\n\ndef padify(b):\n    v= [encode(x[1]) for x in b ]\n    # b is the list of tuples of length batch_size\n    # - first element of a tuple = label\n    # - second = feature (text, sequence)\n    # build vectorized sequence\n    l=max(map(len,v))\n    return(\n    torch.LongTensor([t[0]-1 for t in b]),\n    torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode ='constant',value=0) \n                 for t in v])\n    )\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:51.955635Z","iopub.execute_input":"2024-02-05T06:43:51.956818Z","iopub.status.idle":"2024-02-05T06:43:51.966554Z","shell.execute_reply.started":"2024-02-05T06:43:51.956773Z","shell.execute_reply":"2024-02-05T06:43:51.965585Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"For each tuple in b, it extracts the second element (x[1]), which is the feature (text sequence), and applies the encode function to vectorize the text sequence. The result is a list of vectorized sequences.","metadata":{}},{"cell_type":"markdown","source":"It computes the maximum length (l) among all the vectorized sequences in the minibatch. The map(len, v) applies the len function to each vectorized sequence in v, and max finds the maximum length.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"first_sentence = train_dataset[0][1]\nsecond_sentence = train_dataset[1][1]\n\nf_tokens = encode(first_sentence)\ns_tokens = encode(second_sentence)\n\nprint(f'First sentence in dataset:\\n{first_sentence}')\nprint('Length:', len(train_dataset[0][1]), '\\n')\nprint(f'\\nSecond sentence in dataset:\\n{second_sentence}')\nprint('Length:', len(train_dataset[1][1]), '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:43:51.967982Z","iopub.execute_input":"2024-02-05T06:43:51.969225Z","iopub.status.idle":"2024-02-05T06:44:00.793297Z","shell.execute_reply.started":"2024-02-05T06:43:51.969190Z","shell.execute_reply":"2024-02-05T06:44:00.792447Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"First sentence in dataset:\nWall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\nLength: 144 \n\n\nSecond sentence in dataset:\nCarlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\nLength: 266 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size =len(vocab)\nlabels, features = padify(train_dataset)\nprint(f'features:{features}')\n\nprint(f'\\nlength of first sentence: {len(f_tokens)}')\nprint(f'length of second sentence: {len(s_tokens)}')\nprint(f'size of features: {features.size()}')","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:44:00.794499Z","iopub.execute_input":"2024-02-05T06:44:00.795104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbedClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = torch.nn.Linear(embed_dim, num_class)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.mean(x, dim=1) # torch.mean() computes the mean of all elements in a tensor, it is a reduction operation\n        return self.fc(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def offsetify(b):\n    x = [torch.tensor(encode(t[1])) for t in b]\n    o = [0] + [len(t) for t in x]\n    o = torch.tensor(o[:-1]).cumsum(dim=0)\n    \n    return(\n    torch.LongTensor([t[0]-1 for t in b]),\n    torch.cat(x),\n    o\n    )\n\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=offsetify)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, features, offset = offsetify(train_dataset)\nprint(f'offset:{offset}')\nprint(f'\\nlength of first sentence: {len(f_tokens)}')\nprint(f'length of second sentence: {len(s_tokens)}')\nprint(f'size of data vector: {features.size()}')\nprint(f'size of offset vector: {offset.size()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = EmbedClassifier(vocab_size,32,len(classes)).to(torch_device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch_emb(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n    optimizer  = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n    loss_fn = loss_fn.to(torch_device)\n    net.train()\n    total_loss, acc, count, i = 0,0,0,0\n    for labels, text,off in dataloader:\n        optimizer.zero_grad()\n        labels, text, off = labels.to(torch_device), text.to(torch_device), off.to(torch_device)\n        output = net(text, off)\n        loss = loss_fn(output, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss\n        _, predicted = torch.max(output,1) # torch.max() is used to get the max value of a tensor\n        acc += (predicted == labels).sum()\n        count += len(labels)\n        i += 1\n        if i%report_freq == 0:\n            print(f'iteration {count}, loss {total_loss.item()/count}, accuracy {acc.item()/count}')\n        if epoch_size and count >= epoch_size:\n            break\n    return total_loss.item()/count, acc.item()/count\n\ntrain_epoch_emb(net, train_loader, lr=4, epoch_size=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}